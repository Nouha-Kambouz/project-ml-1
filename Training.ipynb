{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d6d2cb4-7069-4952-8487-b8ba9a8d8757",
   "metadata": {},
   "source": [
    "# Separate Semi-Supervised Training Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2e0947d-bfa2-49bb-818d-08a37251a81c",
   "metadata": {},
   "source": [
    "## 1. Pseudo-Labeling Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2482a8a5-a56b-498c-840b-1ce177c096b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ PSEUDO-LABELING POUR CLASSIFICATION DE TEINT DE PEAU\n",
      "============================================================\n",
      "üìÇ Chargement des donn√©es pr√©trait√©es...\n",
      "  ‚úì Donn√©es charg√©es: 5748 train, 1228 val, 1236 test, 19164 unlabelled\n",
      "üöÄ PSEUDO-LABELING: SESSIONS MULTIPLES\n",
      "==================================================\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec rf, seuil=0.7\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 2402 pseudo-labels, 16762 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 1172 pseudo-labels, 15590 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 594 pseudo-labels, 14996 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 358 pseudo-labels, 14638 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 277 pseudo-labels, 14361 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 201 pseudo-labels, 14160 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 150 pseudo-labels, 14010 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 112 pseudo-labels, 13898 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 106 pseudo-labels, 13792 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 83 pseudo-labels, 13709 restants\n",
      "  ‚úÖ Test accuracy: 0.6675, F1: 0.6569\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec rf, seuil=0.8\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 679 pseudo-labels, 18485 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 569 pseudo-labels, 17916 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 407 pseudo-labels, 17509 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 287 pseudo-labels, 17222 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 252 pseudo-labels, 16970 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 189 pseudo-labels, 16781 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 133 pseudo-labels, 16648 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 122 pseudo-labels, 16526 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 101 pseudo-labels, 16425 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 82 pseudo-labels, 16343 restants\n",
      "  ‚úÖ Test accuracy: 0.6804, F1: 0.6753\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec rf, seuil=0.9\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 60 pseudo-labels, 19104 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 55 pseudo-labels, 19049 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 44 pseudo-labels, 19005 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 34 pseudo-labels, 18971 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 35 pseudo-labels, 18936 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 29 pseudo-labels, 18907 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 27 pseudo-labels, 18880 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 20 pseudo-labels, 18860 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 22 pseudo-labels, 18838 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 21 pseudo-labels, 18817 restants\n",
      "  ‚úÖ Test accuracy: 0.7015, F1: 0.6986\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec svm, seuil=0.7\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 9231 pseudo-labels, 9933 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 4357 pseudo-labels, 5576 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 2039 pseudo-labels, 3537 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 775 pseudo-labels, 2762 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 312 pseudo-labels, 2450 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 149 pseudo-labels, 2301 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 86 pseudo-labels, 2215 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 67 pseudo-labels, 2148 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 48 pseudo-labels, 2100 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 24 pseudo-labels, 2076 restants\n",
      "  ‚úÖ Test accuracy: 0.7362, F1: 0.7341\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec svm, seuil=0.8\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 7160 pseudo-labels, 12004 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 1508 pseudo-labels, 10496 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 1229 pseudo-labels, 9267 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 1377 pseudo-labels, 7890 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 1252 pseudo-labels, 6638 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 788 pseudo-labels, 5850 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 439 pseudo-labels, 5411 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 260 pseudo-labels, 5151 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 143 pseudo-labels, 5008 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 85 pseudo-labels, 4923 restants\n",
      "  ‚úÖ Test accuracy: 0.7233, F1: 0.7212\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec svm, seuil=0.9\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 5684 pseudo-labels, 13480 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 1051 pseudo-labels, 12429 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 406 pseudo-labels, 12023 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 181 pseudo-labels, 11842 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 79 pseudo-labels, 11763 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 29 pseudo-labels, 11734 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 14 pseudo-labels, 11720 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 12 pseudo-labels, 11708 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 16 pseudo-labels, 11692 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 8 pseudo-labels, 11684 restants\n",
      "  ‚úÖ Test accuracy: 0.7273, F1: 0.7235\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec lr, seuil=0.7\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 6256 pseudo-labels, 12908 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 4159 pseudo-labels, 8749 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 1931 pseudo-labels, 6818 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 788 pseudo-labels, 6030 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 359 pseudo-labels, 5671 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 180 pseudo-labels, 5491 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 95 pseudo-labels, 5396 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 56 pseudo-labels, 5340 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 39 pseudo-labels, 5301 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 27 pseudo-labels, 5274 restants\n",
      "  ‚úÖ Test accuracy: 0.6165, F1: 0.6116\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec lr, seuil=0.8\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 3215 pseudo-labels, 15949 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 2076 pseudo-labels, 13873 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 1323 pseudo-labels, 12550 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 828 pseudo-labels, 11722 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 514 pseudo-labels, 11208 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 331 pseudo-labels, 10877 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 228 pseudo-labels, 10649 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 157 pseudo-labels, 10492 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 107 pseudo-labels, 10385 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 74 pseudo-labels, 10311 restants\n",
      "  ‚úÖ Test accuracy: 0.6246, F1: 0.6191\n",
      "\n",
      "üîÑ PSEUDO-LABELING avec lr, seuil=0.9\n",
      "  It√©ration 1/10\n",
      "    Ajout√© 1046 pseudo-labels, 18118 restants\n",
      "  It√©ration 2/10\n",
      "    Ajout√© 569 pseudo-labels, 17549 restants\n",
      "  It√©ration 3/10\n",
      "    Ajout√© 315 pseudo-labels, 17234 restants\n",
      "  It√©ration 4/10\n",
      "    Ajout√© 156 pseudo-labels, 17078 restants\n",
      "  It√©ration 5/10\n",
      "    Ajout√© 90 pseudo-labels, 16988 restants\n",
      "  It√©ration 6/10\n",
      "    Ajout√© 51 pseudo-labels, 16937 restants\n",
      "  It√©ration 7/10\n",
      "    Ajout√© 32 pseudo-labels, 16905 restants\n",
      "  It√©ration 8/10\n",
      "    Ajout√© 23 pseudo-labels, 16882 restants\n",
      "  It√©ration 9/10\n",
      "    Ajout√© 16 pseudo-labels, 16866 restants\n",
      "  It√©ration 10/10\n",
      "    Ajout√© 10 pseudo-labels, 16856 restants\n",
      "  ‚úÖ Test accuracy: 0.6254, F1: 0.6237\n",
      "\n",
      "üíæ R√©sultats sauvegard√©s dans pseudo_labeling_results.pkl\n",
      "\n",
      "üìä R√âSUM√â PSEUDO-LABELING\n",
      "==============================\n",
      "  Exp√©riences: 9\n",
      "  Accuracy moyenne: 0.6781\n",
      "  Meilleur r√©sultat: 0.7362\n",
      "  Meilleure config: svm + seuil 0.7\n",
      "\n",
      "‚úÖ Pseudo-labeling termin√©!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class PseudoLabelingTrainer:\n",
    "    \"\"\"Entra√Æneur pour pseudo-labeling\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='preprocessed_ml_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = ['dark', 'light', 'mid-dark', 'mid-light']\n",
    "        self.load_data()\n",
    "        self.results = []\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les donn√©es pr√©trait√©es\"\"\"\n",
    "        print(\"üìÇ Chargement des donn√©es pr√©trait√©es...\")\n",
    "        \n",
    "        # Charger les datasets\n",
    "        datasets = {}\n",
    "        for split in ['train', 'val', 'test', 'unlabelled']:\n",
    "            data = np.load(os.path.join(self.data_dir, f'{split}_data.npz'))\n",
    "            datasets[split] = {\n",
    "                'X': data['X'],\n",
    "                'y': data['y'],\n",
    "                'filenames': data['filenames']\n",
    "            }\n",
    "        \n",
    "        self.X_train = datasets['train']['X']\n",
    "        self.y_train = datasets['train']['y']\n",
    "        self.X_val = datasets['val']['X'] \n",
    "        self.y_val = datasets['val']['y']\n",
    "        self.X_test = datasets['test']['X']\n",
    "        self.y_test = datasets['test']['y']\n",
    "        self.X_unlabelled = datasets['unlabelled']['X']\n",
    "        \n",
    "        print(f\"  ‚úì Donn√©es charg√©es: {len(self.X_train)} train, {len(self.X_val)} val, {len(self.X_test)} test, {len(self.X_unlabelled)} unlabelled\")\n",
    "    \n",
    "    def train_pseudo_labeling(self, base_classifier='rf', confidence_threshold=0.8, max_iterations=10):\n",
    "        \"\"\"Impl√©mente le pseudo-labeling\"\"\"\n",
    "        print(f\"\\nüîÑ PSEUDO-LABELING avec {base_classifier}, seuil={confidence_threshold}\")\n",
    "        \n",
    "        # S√©lectionner le classifieur de base\n",
    "        if base_classifier == 'rf':\n",
    "            clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        elif base_classifier == 'svm':\n",
    "            clf = SVC(probability=True, random_state=42)\n",
    "        elif base_classifier == 'lr':\n",
    "            clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        \n",
    "        # Donn√©es initiales\n",
    "        X_labeled = self.X_train.copy()\n",
    "        y_labeled = self.y_train.copy()\n",
    "        X_unlabeled = self.X_unlabelled.copy()\n",
    "        \n",
    "        best_accuracy = 0\n",
    "        iteration_results = []\n",
    "        \n",
    "        for iteration in range(max_iterations):\n",
    "            print(f\"  It√©ration {iteration + 1}/{max_iterations}\")\n",
    "            \n",
    "            # Entra√Æner sur donn√©es labellis√©es actuelles\n",
    "            clf.fit(X_labeled, y_labeled)\n",
    "            \n",
    "            # Pr√©dire sur donn√©es non-labellis√©es\n",
    "            if hasattr(clf, 'predict_proba'):\n",
    "                proba_predictions = clf.predict_proba(X_unlabeled)\n",
    "                predictions = np.argmax(proba_predictions, axis=1)\n",
    "                confidences = np.max(proba_predictions, axis=1)\n",
    "            else:\n",
    "                predictions = clf.predict(X_unlabeled)\n",
    "                confidences = np.ones(len(predictions))  # Pas de probabilit√©s\n",
    "            \n",
    "            # S√©lectionner les pr√©dictions confiantes\n",
    "            confident_mask = confidences >= confidence_threshold\n",
    "            X_confident = X_unlabeled[confident_mask]\n",
    "            y_confident = predictions[confident_mask]\n",
    "            \n",
    "            if len(X_confident) == 0:\n",
    "                print(f\"    Aucune pr√©diction confiante trouv√©e, arr√™t √† l'it√©ration {iteration + 1}\")\n",
    "                break\n",
    "            \n",
    "            # Ajouter les donn√©es pseudo-labellis√©es\n",
    "            X_labeled = np.vstack([X_labeled, X_confident])\n",
    "            y_labeled = np.hstack([y_labeled, y_confident])\n",
    "            \n",
    "            # Retirer des donn√©es non-labellis√©es\n",
    "            X_unlabeled = X_unlabeled[~confident_mask]\n",
    "            \n",
    "            print(f\"    Ajout√© {len(X_confident)} pseudo-labels, {len(X_unlabeled)} restants\")\n",
    "            \n",
    "            # √âvaluer sur validation\n",
    "            val_accuracy = clf.score(self.X_val, self.y_val)\n",
    "            iteration_results.append({\n",
    "                'iteration': iteration + 1,\n",
    "                'val_accuracy': val_accuracy,\n",
    "                'pseudo_labels_added': len(X_confident),\n",
    "                'remaining_unlabeled': len(X_unlabeled)\n",
    "            })\n",
    "            \n",
    "            if val_accuracy > best_accuracy:\n",
    "                best_accuracy = val_accuracy\n",
    "        \n",
    "        # √âvaluation finale sur test\n",
    "        test_accuracy = clf.score(self.X_test, self.y_test)\n",
    "        test_f1 = f1_score(self.y_test, clf.predict(self.X_test), average='weighted')\n",
    "        \n",
    "        result = {\n",
    "            'method': 'pseudo_labeling',\n",
    "            'base_classifier': base_classifier,\n",
    "            'confidence_threshold': confidence_threshold,\n",
    "            'max_iterations': max_iterations,\n",
    "            'final_test_accuracy': test_accuracy,\n",
    "            'final_test_f1': test_f1,\n",
    "            'best_val_accuracy': best_accuracy,\n",
    "            'iterations': iteration_results,\n",
    "            'total_pseudo_labels': len(X_labeled) - len(self.X_train)\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(f\"  ‚úÖ Test accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_multiple_sessions(self):\n",
    "        \"\"\"Ex√©cute plusieurs sessions avec diff√©rents param√®tres\"\"\"\n",
    "        print(\"üöÄ PSEUDO-LABELING: SESSIONS MULTIPLES\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for clf in ['rf', 'svm', 'lr']:\n",
    "            for threshold in [0.7, 0.8, 0.9]:\n",
    "                self.train_pseudo_labeling(base_classifier=clf, confidence_threshold=threshold)\n",
    "    \n",
    "    def save_results(self, output_file='pseudo_labeling_results.pkl'):\n",
    "        \"\"\"Sauvegarde les r√©sultats\"\"\"\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        print(f\"\\nüíæ R√©sultats sauvegard√©s dans {output_file}\")\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Affiche un r√©sum√©\"\"\"\n",
    "        print(\"\\nüìä R√âSUM√â PSEUDO-LABELING\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"Aucun r√©sultat\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        print(f\"  Exp√©riences: {len(df)}\")\n",
    "        print(f\"  Accuracy moyenne: {df['final_test_accuracy'].mean():.4f}\")\n",
    "        print(f\"  Meilleur r√©sultat: {df['final_test_accuracy'].max():.4f}\")\n",
    "        \n",
    "        # Meilleur r√©sultat\n",
    "        best_idx = df['final_test_accuracy'].idxmax()\n",
    "        best = df.loc[best_idx]\n",
    "        print(f\"  Meilleure config: {best['base_classifier']} + seuil {best['confidence_threshold']}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour pseudo-labeling\"\"\"\n",
    "    print(\"üéØ PSEUDO-LABELING POUR CLASSIFICATION DE TEINT DE PEAU\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trainer = PseudoLabelingTrainer(data_dir='preprocessed_ml_data')\n",
    "    trainer.run_multiple_sessions()\n",
    "    trainer.save_results('pseudo_labeling_results.pkl')\n",
    "    trainer.display_summary()\n",
    "    \n",
    "    print(\"\\n‚úÖ Pseudo-labeling termin√©!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73012381-0d84-4257-af4d-c91f6813a196",
   "metadata": {},
   "source": [
    "## 2. Graph-Based Methods Algorithm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "416c8c89-4b17-4199-80a9-fc7b19fd8a63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ GRAPH-BASED METHODS POUR CLASSIFICATION DE TEINT DE PEAU\n",
      "============================================================\n",
      "üìÇ Chargement des donn√©es pr√©trait√©es...\n",
      "  ‚úì Donn√©es charg√©es: 5748 train, 1228 val, 1236 test, 19164 unlabelled\n",
      "üöÄ GRAPH-BASED METHODS: SESSIONS MULTIPLES\n",
      "==================================================\n",
      "\n",
      "üï∏Ô∏è  PROPAGATION: kernel=rbf, gamma=10\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 34.67s\n",
      "\n",
      "üï∏Ô∏è  PROPAGATION: kernel=rbf, gamma=20\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 23.45s\n",
      "\n",
      "üï∏Ô∏è  PROPAGATION: kernel=rbf, gamma=50\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 22.56s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=10, alpha=0.1\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 70.25s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=10, alpha=0.2\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 92.23s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=10, alpha=0.5\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 218.44s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=20, alpha=0.1\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 70.38s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=20, alpha=0.2\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 105.88s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=20, alpha=0.5\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 197.34s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=50, alpha=0.1\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 76.65s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=50, alpha=0.2\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 92.81s\n",
      "\n",
      "üï∏Ô∏è  SPREADING: kernel=rbf, gamma=50, alpha=0.5\n",
      "  ‚úÖ Test accuracy: 0.2702, F1: 0.1393, Time: 210.11s\n",
      "\n",
      "üíæ R√©sultats sauvegard√©s dans graph_based_results.pkl\n",
      "\n",
      "üìä R√âSUM√â GRAPH-BASED METHODS\n",
      "==============================\n",
      "  Exp√©riences: 12\n",
      "  Accuracy moyenne: 0.2702\n",
      "  Meilleur r√©sultat: 0.2702\n",
      "  Meilleure config: graph_propagation + gamma 10 + alpha nan\n",
      "\n",
      "‚úÖ Graph-based methods termin√©!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class GraphBasedTrainer:\n",
    "    \"\"\"Entra√Æneur pour m√©thodes graph-based\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='preprocessed_ml_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = ['dark', 'light', 'mid-dark', 'mid-light']\n",
    "        self.load_data()\n",
    "        self.results = []\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les donn√©es pr√©trait√©es\"\"\"\n",
    "        print(\"üìÇ Chargement des donn√©es pr√©trait√©es...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        for split in ['train', 'val', 'test', 'unlabelled']:\n",
    "            data = np.load(os.path.join(self.data_dir, f'{split}_data.npz'))\n",
    "            datasets[split] = {\n",
    "                'X': data['X'],\n",
    "                'y': data['y'],\n",
    "                'filenames': data['filenames']\n",
    "            }\n",
    "        \n",
    "        self.X_train = datasets['train']['X']\n",
    "        self.y_train = datasets['train']['y']\n",
    "        self.X_val = datasets['val']['X'] \n",
    "        self.y_val = datasets['val']['y']\n",
    "        self.X_test = datasets['test']['X']\n",
    "        self.y_test = datasets['test']['y']\n",
    "        self.X_unlabelled = datasets['unlabelled']['X']\n",
    "        \n",
    "        print(f\"  ‚úì Donn√©es charg√©es: {len(self.X_train)} train, {len(self.X_val)} val, {len(self.X_test)} test, {len(self.X_unlabelled)} unlabelled\")\n",
    "    \n",
    "    def train_graph_method(self, method='propagation', kernel='rbf', gamma=20, alpha=0.2):\n",
    "        \"\"\"Impl√©mente les m√©thodes graph-based\"\"\"\n",
    "        print(f\"\\nüï∏Ô∏è  {method.upper()}: kernel={kernel}, gamma={gamma}\", end=\"\")\n",
    "        if method == 'spreading':\n",
    "            print(f\", alpha={alpha}\")\n",
    "        else:\n",
    "            print(\"\")\n",
    "        \n",
    "        # Combiner donn√©es labellis√©es et non-labellis√©es\n",
    "        X_combined = np.vstack([self.X_train, self.X_unlabelled])\n",
    "        y_combined = np.hstack([self.y_train, np.full(len(self.X_unlabelled), -1)])  # -1 pour non-labellis√©\n",
    "        \n",
    "        # S√©lectionner la m√©thode\n",
    "        if method == 'propagation':\n",
    "            clf = LabelPropagation(kernel=kernel, gamma=gamma, max_iter=1000)\n",
    "        elif method == 'spreading':\n",
    "            clf = LabelSpreading(kernel=kernel, gamma=gamma, alpha=alpha, max_iter=1000)\n",
    "        \n",
    "        # Entra√Æner\n",
    "        start_time = time.time()\n",
    "        clf.fit(X_combined, y_combined)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # Pr√©dire sur test\n",
    "        y_pred = clf.predict(self.X_test)\n",
    "        \n",
    "        # √âvaluer\n",
    "        test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        test_f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        \n",
    "        result = {\n",
    "            'method': f'graph_{method}',\n",
    "            'kernel': kernel,\n",
    "            'gamma': gamma,\n",
    "            'alpha': alpha if method == 'spreading' else None,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_f1': test_f1,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(f\"  ‚úÖ Test accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}, Time: {training_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_multiple_sessions(self):\n",
    "        \"\"\"Ex√©cute plusieurs sessions avec diff√©rents param√®tres\"\"\"\n",
    "        print(\"üöÄ GRAPH-BASED METHODS: SESSIONS MULTIPLES\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for method in ['propagation', 'spreading']:\n",
    "            for gamma in [10, 20, 50]:\n",
    "                if method == 'propagation':\n",
    "                    # LabelPropagation n'a pas de param√®tre alpha\n",
    "                    self.train_graph_method(method=method, gamma=gamma)\n",
    "                else:\n",
    "                    # LabelSpreading a un param√®tre alpha\n",
    "                    for alpha in [0.1, 0.2, 0.5]:\n",
    "                        self.train_graph_method(method=method, gamma=gamma, alpha=alpha)\n",
    "    \n",
    "    def save_results(self, output_file='graph_based_results.pkl'):\n",
    "        \"\"\"Sauvegarde les r√©sultats\"\"\"\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        print(f\"\\nüíæ R√©sultats sauvegard√©s dans {output_file}\")\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Affiche un r√©sum√©\"\"\"\n",
    "        print(\"\\nüìä R√âSUM√â GRAPH-BASED METHODS\")\n",
    "        print(\"=\" * 30)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"Aucun r√©sultat\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        print(f\"  Exp√©riences: {len(df)}\")\n",
    "        print(f\"  Accuracy moyenne: {df['test_accuracy'].mean():.4f}\")\n",
    "        print(f\"  Meilleur r√©sultat: {df['test_accuracy'].max():.4f}\")\n",
    "        \n",
    "        # Meilleur r√©sultat\n",
    "        best_idx = df['test_accuracy'].idxmax()\n",
    "        best = df.loc[best_idx]\n",
    "        config_str = f\"{best['method']} + gamma {best['gamma']}\"\n",
    "        if best['alpha'] is not None:\n",
    "            config_str += f\" + alpha {best['alpha']}\"\n",
    "        print(f\"  Meilleure config: {config_str}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour graph-based methods\"\"\"\n",
    "    print(\"üéØ GRAPH-BASED METHODS POUR CLASSIFICATION DE TEINT DE PEAU\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trainer = GraphBasedTrainer(data_dir='preprocessed_ml_data')\n",
    "    trainer.run_multiple_sessions()\n",
    "    trainer.save_results('graph_based_results.pkl')\n",
    "    trainer.display_summary()\n",
    "    \n",
    "    print(\"\\n‚úÖ Graph-based methods termin√©!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1764eb47-151b-4fa9-ae41-0bf6eb2cc035",
   "metadata": {},
   "source": [
    "## 3. Consistency Regularization Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9568a624-c95f-4923-8b38-8907eb75a5b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ CONSISTENCY REGULARIZATION POUR CLASSIFICATION DE TEINT DE PEAU\n",
      "============================================================\n",
      "üìÇ Chargement des donn√©es pr√©trait√©es...\n",
      "  ‚úì Donn√©es charg√©es: 5748 train, 1228 val, 1236 test, 19164 unlabelled\n",
      "üöÄ CONSISTENCY REGULARIZATION: SESSIONS MULTIPLES\n",
      "==================================================\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 5 estimators, perturbation=0.05\n",
      "  ‚úÖ Test accuracy: 0.5631, F1: 0.5494, Time: 0.31s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 5 estimators, perturbation=0.1\n",
      "  ‚úÖ Test accuracy: 0.5761, F1: 0.5614, Time: 0.33s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 5 estimators, perturbation=0.2\n",
      "  ‚úÖ Test accuracy: 0.5655, F1: 0.5477, Time: 0.34s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 10 estimators, perturbation=0.05\n",
      "  ‚úÖ Test accuracy: 0.5979, F1: 0.5865, Time: 1.41s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 10 estimators, perturbation=0.1\n",
      "  ‚úÖ Test accuracy: 0.5906, F1: 0.5776, Time: 1.33s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 10 estimators, perturbation=0.2\n",
      "  ‚úÖ Test accuracy: 0.6303, F1: 0.6187, Time: 1.33s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 20 estimators, perturbation=0.05\n",
      "  ‚úÖ Test accuracy: 0.6424, F1: 0.6333, Time: 6.28s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 20 estimators, perturbation=0.1\n",
      "  ‚úÖ Test accuracy: 0.6513, F1: 0.6430, Time: 6.22s\n",
      "\n",
      "üîÑ CONSISTENCY REGULARIZATION: 20 estimators, perturbation=0.2\n",
      "  ‚úÖ Test accuracy: 0.6456, F1: 0.6357, Time: 6.05s\n",
      "\n",
      "üíæ R√©sultats sauvegard√©s dans consistency_results.pkl\n",
      "\n",
      "üìä R√âSUM√â CONSISTENCY REGULARIZATION\n",
      "===================================\n",
      "  Exp√©riences: 9\n",
      "  Accuracy moyenne: 0.6070\n",
      "  Meilleur r√©sultat: 0.6513\n",
      "  Meilleure config: 20 estimators + perturbation 0.1\n",
      "\n",
      "‚úÖ Consistency regularization termin√©!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ConsistencyTrainer:\n",
    "    \"\"\"Entra√Æneur pour r√©gularisation de coh√©rence\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='preprocessed_ml_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.classes = ['dark', 'light', 'mid-dark', 'mid-light']\n",
    "        self.load_data()\n",
    "        self.results = []\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les donn√©es pr√©trait√©es\"\"\"\n",
    "        print(\"üìÇ Chargement des donn√©es pr√©trait√©es...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        for split in ['train', 'val', 'test', 'unlabelled']:\n",
    "            data = np.load(os.path.join(self.data_dir, f'{split}_data.npz'))\n",
    "            datasets[split] = {\n",
    "                'X': data['X'],\n",
    "                'y': data['y'],\n",
    "                'filenames': data['filenames']\n",
    "            }\n",
    "        \n",
    "        self.X_train = datasets['train']['X']\n",
    "        self.y_train = datasets['train']['y']\n",
    "        self.X_val = datasets['val']['X'] \n",
    "        self.y_val = datasets['val']['y']\n",
    "        self.X_test = datasets['test']['X']\n",
    "        self.y_test = datasets['test']['y']\n",
    "        self.X_unlabelled = datasets['unlabelled']['X']\n",
    "        \n",
    "        print(f\"  ‚úì Donn√©es charg√©es: {len(self.X_train)} train, {len(self.X_val)} val, {len(self.X_test)} test, {len(self.X_unlabelled)} unlabelled\")\n",
    "    \n",
    "    def train_consistency_regularization(self, n_estimators=10, perturbation_strength=0.1):\n",
    "        \"\"\"Impl√©mente la r√©gularisation de coh√©rence avec ensemble\"\"\"\n",
    "        print(f\"\\nüîÑ CONSISTENCY REGULARIZATION: {n_estimators} estimators, perturbation={perturbation_strength}\")\n",
    "        \n",
    "        # Cr√©er des perturbations des donn√©es d'entra√Ænement\n",
    "        X_perturbed = []\n",
    "        y_perturbed = []\n",
    "        \n",
    "        for _ in range(n_estimators):\n",
    "            # Ajouter du bruit gaussien\n",
    "            noise = np.random.normal(0, perturbation_strength, self.X_train.shape)\n",
    "            X_pert = self.X_train + noise\n",
    "            X_perturbed.append(X_pert)\n",
    "            y_perturbed.append(self.y_train)\n",
    "        \n",
    "        X_perturbed = np.vstack(X_perturbed)\n",
    "        y_perturbed = np.hstack(y_perturbed)\n",
    "        \n",
    "        # Entra√Æner un classifieur d'ensemble\n",
    "        clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=42)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        clf.fit(X_perturbed, y_perturbed)\n",
    "        training_time = time.time() - start_time\n",
    "        \n",
    "        # √âvaluer\n",
    "        y_pred = clf.predict(self.X_test)\n",
    "        test_accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        test_f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        \n",
    "        result = {\n",
    "            'method': 'consistency_regularization',\n",
    "            'n_estimators': n_estimators,\n",
    "            'perturbation_strength': perturbation_strength,\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_f1': test_f1,\n",
    "            'training_time': training_time\n",
    "        }\n",
    "        \n",
    "        self.results.append(result)\n",
    "        print(f\"  ‚úÖ Test accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}, Time: {training_time:.2f}s\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def run_multiple_sessions(self):\n",
    "        \"\"\"Ex√©cute plusieurs sessions avec diff√©rents param√®tres\"\"\"\n",
    "        print(\"üöÄ CONSISTENCY REGULARIZATION: SESSIONS MULTIPLES\")\n",
    "        print(\"=\" * 50)\n",
    "        \n",
    "        for n_est in [5, 10, 20]:\n",
    "            for pert in [0.05, 0.1, 0.2]:\n",
    "                self.train_consistency_regularization(n_estimators=n_est, perturbation_strength=pert)\n",
    "    \n",
    "    def save_results(self, output_file='consistency_results.pkl'):\n",
    "        \"\"\"Sauvegarde les r√©sultats\"\"\"\n",
    "        with open(output_file, 'wb') as f:\n",
    "            pickle.dump(self.results, f)\n",
    "        print(f\"\\nüíæ R√©sultats sauvegard√©s dans {output_file}\")\n",
    "    \n",
    "    def display_summary(self):\n",
    "        \"\"\"Affiche un r√©sum√©\"\"\"\n",
    "        print(\"\\nüìä R√âSUM√â CONSISTENCY REGULARIZATION\")\n",
    "        print(\"=\" * 35)\n",
    "        \n",
    "        if not self.results:\n",
    "            print(\"Aucun r√©sultat\")\n",
    "            return\n",
    "        \n",
    "        df = pd.DataFrame(self.results)\n",
    "        print(f\"  Exp√©riences: {len(df)}\")\n",
    "        print(f\"  Accuracy moyenne: {df['test_accuracy'].mean():.4f}\")\n",
    "        print(f\"  Meilleur r√©sultat: {df['test_accuracy'].max():.4f}\")\n",
    "        \n",
    "        # Meilleur r√©sultat\n",
    "        best_idx = df['test_accuracy'].idxmax()\n",
    "        best = df.loc[best_idx]\n",
    "        print(f\"  Meilleure config: {best['n_estimators']} estimators + perturbation {best['perturbation_strength']}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour consistency regularization\"\"\"\n",
    "    print(\"üéØ CONSISTENCY REGULARIZATION POUR CLASSIFICATION DE TEINT DE PEAU\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    trainer = ConsistencyTrainer(data_dir='preprocessed_ml_data')\n",
    "    trainer.run_multiple_sessions()\n",
    "    trainer.save_results('consistency_results.pkl')\n",
    "    trainer.display_summary()\n",
    "    \n",
    "    print(\"\\n‚úÖ Consistency regularization termin√©!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c41f4f-66a5-4ebb-9a76-763ad09c3484",
   "metadata": {},
   "source": [
    "# save the best model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a3537a97-f17c-403b-8537-533f15d53fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéØ S√âLECTION ET SAUVEGARDE DU MEILLEUR MOD√àLE\n",
      "============================================================\n",
      "üìÇ Chargement des donn√©es pr√©trait√©es...\n",
      "  ‚úì Donn√©es charg√©es: 5748 train, 1228 val, 1236 test, 19164 unlabelled\n",
      "‚úì Charg√© 9 r√©sultats depuis pseudo_labeling_results.pkl\n",
      "‚úì Charg√© 12 r√©sultats depuis graph_based_results.pkl\n",
      "‚úì Charg√© 9 r√©sultats depuis consistency_results.pkl\n",
      "\n",
      "üèÜ MEILLEUR MOD√àLE TROUV√â:\n",
      "  M√©thode: pseudo_labeling\n",
      "  Accuracy de test: 0.7362\n",
      "  F1-score: 0.7341\n",
      "  Param√®tres: {'base_classifier': 'svm', 'confidence_threshold': 0.7, 'max_iterations': 10, 'best_val_accuracy': 0.738599348534202, 'total_pseudo_labels': 17088}\n",
      "\n",
      "üîÑ R√©entra√Ænement du mod√®le: pseudo_labeling\n",
      "  ‚úÖ Mod√®le r√©entra√Æn√© - Test accuracy: 0.7257, F1: 0.7250\n",
      "üíæ Mod√®le sauvegard√© dans best_model.pkl\n",
      "\n",
      "‚úÖ Meilleur mod√®le pr√™t pour le d√©ploiement!\n",
      "Vous pouvez maintenant lancer l'application Streamlit.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "import time\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.semi_supervised import LabelPropagation, LabelSpreading\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class ModelSelector:\n",
    "    \"\"\"S√©lecteur et sauvegardeur du meilleur mod√®le\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir='preprocessed_ml_data'):\n",
    "        self.data_dir = data_dir\n",
    "        self.load_data()\n",
    "    \n",
    "    def load_data(self):\n",
    "        \"\"\"Charge les donn√©es pr√©trait√©es\"\"\"\n",
    "        print(\"üìÇ Chargement des donn√©es pr√©trait√©es...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        for split in ['train', 'val', 'test', 'unlabelled']:\n",
    "            data = np.load(os.path.join(self.data_dir, f'{split}_data.npz'))\n",
    "            datasets[split] = {\n",
    "                'X': data['X'],\n",
    "                'y': data['y'],\n",
    "                'filenames': data['filenames']\n",
    "            }\n",
    "        \n",
    "        self.X_train = datasets['train']['X']\n",
    "        self.y_train = datasets['train']['y']\n",
    "        self.X_val = datasets['val']['X'] \n",
    "        self.y_val = datasets['val']['y']\n",
    "        self.X_test = datasets['test']['X']\n",
    "        self.y_test = datasets['test']['y']\n",
    "        self.X_unlabelled = datasets['unlabelled']['X']\n",
    "        \n",
    "        print(f\"  ‚úì Donn√©es charg√©es: {len(self.X_train)} train, {len(self.X_val)} val, {len(self.X_test)} test, {len(self.X_unlabelled)} unlabelled\")\n",
    "    \n",
    "    def load_and_compare_results(self):\n",
    "        \"\"\"Charge et compare tous les r√©sultats d'entra√Ænement\"\"\"\n",
    "        result_files = [\n",
    "            'pseudo_labeling_results.pkl',\n",
    "            'graph_based_results.pkl', \n",
    "            'consistency_results.pkl'\n",
    "        ]\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for file in result_files:\n",
    "            if os.path.exists(file):\n",
    "                with open(file, 'rb') as f:\n",
    "                    results = pickle.load(f)\n",
    "                    all_results.extend(results)\n",
    "                    print(f\"‚úì Charg√© {len(results)} r√©sultats depuis {file}\")\n",
    "            else:\n",
    "                print(f\"‚ö†Ô∏è Fichier {file} non trouv√©\")\n",
    "        \n",
    "        if not all_results:\n",
    "            print(\"‚ùå Aucun r√©sultat trouv√©!\")\n",
    "            return None\n",
    "        \n",
    "        return all_results\n",
    "    \n",
    "    def select_best_model(self, all_results):\n",
    "        \"\"\"S√©lectionne le meilleur mod√®le bas√© sur l'accuracy de test\"\"\"\n",
    "        # Fonction pour obtenir l'accuracy (g√®re les cl√©s diff√©rentes)\n",
    "        def get_accuracy(result):\n",
    "            if 'test_accuracy' in result:\n",
    "                return result['test_accuracy']\n",
    "            elif 'final_test_accuracy' in result:\n",
    "                return result['final_test_accuracy']\n",
    "            else:\n",
    "                return 0  # Valeur par d√©faut si pas trouv√©\n",
    "        \n",
    "        best_result = max(all_results, key=get_accuracy)\n",
    "        \n",
    "        # Obtenir l'accuracy pour l'affichage\n",
    "        accuracy = get_accuracy(best_result)\n",
    "        f1_score_val = best_result.get('test_f1', best_result.get('final_test_f1', 0))\n",
    "        \n",
    "        print(\"\\nüèÜ MEILLEUR MOD√àLE TROUV√â:\")\n",
    "        print(f\"  M√©thode: {best_result['method']}\")\n",
    "        print(f\"  Accuracy de test: {accuracy:.4f}\")\n",
    "        print(f\"  F1-score: {f1_score_val:.4f}\")\n",
    "        \n",
    "        # Afficher les param√®tres sp√©cifiques\n",
    "        params = {k: v for k, v in best_result.items() \n",
    "                 if k not in ['method', 'test_accuracy', 'final_test_accuracy', 'test_f1', 'final_test_f1', 'training_time', 'iterations']}\n",
    "        print(f\"  Param√®tres: {params}\")\n",
    "        \n",
    "        return best_result\n",
    "    \n",
    "    def retrain_best_model(self, best_result):\n",
    "        \"\"\"R√©entra√Æne le meilleur mod√®le avec tous les param√®tres optimaux\"\"\"\n",
    "        method = best_result['method']\n",
    "        print(f\"\\nüîÑ R√©entra√Ænement du mod√®le: {method}\")\n",
    "        \n",
    "        if method.startswith('pseudo_labeling'):\n",
    "            # Pseudo-labeling\n",
    "            base_classifier = best_result['base_classifier']\n",
    "            confidence_threshold = best_result['confidence_threshold']\n",
    "            \n",
    "            if base_classifier == 'rf':\n",
    "                clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "            elif base_classifier == 'svm':\n",
    "                clf = SVC(probability=True, random_state=42)\n",
    "            elif base_classifier == 'lr':\n",
    "                clf = LogisticRegression(random_state=42, max_iter=1000)\n",
    "            \n",
    "            # Entra√Æner sur les donn√©es labellis√©es\n",
    "            clf.fit(self.X_train, self.y_train)\n",
    "            \n",
    "        elif method.startswith('graph_'):\n",
    "            # Graph-based\n",
    "            method_type = method.split('_')[1]  # 'propagation' or 'spreading'\n",
    "            kernel = best_result['kernel']\n",
    "            gamma = best_result['gamma']\n",
    "            \n",
    "            X_combined = np.vstack([self.X_train, self.X_unlabelled])\n",
    "            y_combined = np.hstack([self.y_train, np.full(len(self.X_unlabelled), -1)])\n",
    "            \n",
    "            if method_type == 'propagation':\n",
    "                clf = LabelPropagation(kernel=kernel, gamma=gamma, max_iter=1000)\n",
    "            elif method_type == 'spreading':\n",
    "                alpha = best_result.get('alpha', 0.2)\n",
    "                clf = LabelSpreading(kernel=kernel, gamma=gamma, alpha=alpha, max_iter=1000)\n",
    "            \n",
    "            clf.fit(X_combined, y_combined)\n",
    "            \n",
    "        elif method == 'consistency_regularization':\n",
    "            # Consistency regularization\n",
    "            n_estimators = best_result['n_estimators']\n",
    "            perturbation_strength = best_result['perturbation_strength']\n",
    "            \n",
    "            # Cr√©er des perturbations\n",
    "            X_perturbed = []\n",
    "            y_perturbed = []\n",
    "            \n",
    "            for _ in range(n_estimators):\n",
    "                noise = np.random.normal(0, perturbation_strength, self.X_train.shape)\n",
    "                X_pert = self.X_train + noise\n",
    "                X_perturbed.append(X_pert)\n",
    "                y_perturbed.append(self.y_train)\n",
    "            \n",
    "            X_perturbed = np.vstack(X_perturbed)\n",
    "            y_perturbed = np.hstack(y_perturbed)\n",
    "            \n",
    "            clf = ExtraTreesClassifier(n_estimators=n_estimators, random_state=42)\n",
    "            clf.fit(X_perturbed, y_perturbed)\n",
    "        \n",
    "        # Validation finale\n",
    "        test_accuracy = clf.score(self.X_test, self.y_test)\n",
    "        test_f1 = f1_score(self.y_test, clf.predict(self.X_test), average='weighted')\n",
    "        \n",
    "        print(f\"  ‚úÖ Mod√®le r√©entra√Æn√© - Test accuracy: {test_accuracy:.4f}, F1: {test_f1:.4f}\")\n",
    "        \n",
    "        return clf\n",
    "    \n",
    "    def save_best_model(self, model, filename='best_model.pkl'):\n",
    "        \"\"\"Sauvegarde le meilleur mod√®le\"\"\"\n",
    "        with open(filename, 'wb') as f:\n",
    "            pickle.dump(model, f)\n",
    "        print(f\"üíæ Mod√®le sauvegard√© dans {filename}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale pour s√©lection et sauvegarde du meilleur mod√®le\"\"\"\n",
    "    print(\"üéØ S√âLECTION ET SAUVEGARDE DU MEILLEUR MOD√àLE\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Initialiser le s√©lecteur\n",
    "    selector = ModelSelector(data_dir='preprocessed_ml_data')\n",
    "    \n",
    "    # Charger et comparer les r√©sultats\n",
    "    all_results = selector.load_and_compare_results()\n",
    "    \n",
    "    if all_results:\n",
    "        # S√©lectionner le meilleur\n",
    "        best_result = selector.select_best_model(all_results)\n",
    "        \n",
    "        # R√©entra√Æner le meilleur mod√®le\n",
    "        best_model = selector.retrain_best_model(best_result)\n",
    "        \n",
    "        # Sauvegarder\n",
    "        selector.save_best_model(best_model, 'best_model.pkl')\n",
    "        \n",
    "        print(\"\\n‚úÖ Meilleur mod√®le pr√™t pour le d√©ploiement!\")\n",
    "        print(\"Vous pouvez maintenant lancer l'application Streamlit.\")\n",
    "    \n",
    "    else:\n",
    "        print(\"‚ùå Impossible de trouver des r√©sultats d'entra√Ænement.\")\n",
    "        print(\"Assurez-vous d'avoir ex√©cut√© les scripts d'entra√Ænement.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8acbe7c-807f-42a3-aff1-f2546af33a9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
