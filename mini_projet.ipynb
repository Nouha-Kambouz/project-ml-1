{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "030a92f7-ff59-417c-aeb5-da3b90fdc4e5",
   "metadata": {},
   "source": [
    "## ğŸ“Š SCRIPT DE CRÃ‰ATION DE DATASET SEMI-SUPERVISÃ‰\n",
    "\n",
    "---\n",
    "\n",
    "Ce script est conÃ§u pour gÃ©nÃ©rer un jeu de donnÃ©es optimisÃ© pour les modÃ¨les d'**apprentissage semi-supervisÃ© (Semi-Supervised Learning)**, en respectant des proportions et un Ã©quilibrage stricts.\n",
    "\n",
    "### ğŸ“ ParamÃ¨tres ClÃ©s du Dataset\n",
    "\n",
    "| ParamÃ¨tre | Valeur | Objectif |\n",
    "| :--- | :--- | :--- |\n",
    "| **Ratio de Supervision** | **30% / 70%** | Utilisation maximale de donnÃ©es non-labellisÃ©es. |\n",
    "| **Images LabellisÃ©es** | 30% | DonnÃ©es de dÃ©part pour l'entraÃ®nement du modÃ¨le. |\n",
    "| **Images Non-LabellisÃ©es** | 70% | Utilisation pour les techniques de *Pseudo-Labeling* ou de cohÃ©rence. |\n",
    "| **Taille des Classes** | **6 844 images** | Assurer un Ã©quilibre parfait entre toutes les classes (minimum commun). |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ’¡ Note d'ExÃ©cution\n",
    "\n",
    "Pour exÃ©cuter la crÃ©ation de ce dataset, veuillez vous assurer que la fonction de sous-Ã©chantillonnage (down-sampling) est configurÃ©e pour cibler le nombre minimum commun de **6 844 images** par classe pour garantir l'Ã©quilibrage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2915e97-b969-4274-8da2-c7d29f6dfcdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ CRÃ‰ATEUR DE DATASET SEMI-SUPERVISÃ‰\n",
      "Ratio: 30% labellisÃ© / 70% non-labellisÃ©\n",
      "============================================================\n",
      "============================================================\n",
      "CONFIGURATION DU DATASET SEMI-SUPERVISÃ‰\n",
      "============================================================\n",
      "Images par classe: 6,844\n",
      "Ratio: 30% labellisÃ© / 70% non-labellisÃ©\n",
      "\n",
      "DÃ©tails par classe:\n",
      "  LabellisÃ©: 2,053 images\n",
      "    â”œâ”€â”€ Train: 1,437 (70%)\n",
      "    â”œâ”€â”€ Validation: 307 (15%)\n",
      "    â””â”€â”€ Test: 309 (15%)\n",
      "  Non-labellisÃ©: 4,791 images\n",
      "============================================================\n",
      "ğŸš€ DÃ‰BUT DE LA CRÃ‰ATION DU DATASET SEMI-SUPERVISÃ‰\n",
      "============================================================\n",
      "\n",
      "ğŸ” VÃ‰RIFICATION DES DONNÃ‰ES SOURCE\n",
      "  âœ“ dark: 8,640 images\n",
      "  âœ“ light: 9,769 images\n",
      "  âœ“ mid-dark: 10,576 images\n",
      "  âœ“ mid-light: 6,844 images\n",
      "\n",
      "ğŸ“ CRÃ‰ATION DE LA STRUCTURE DES DOSSIERS\n",
      "  âœ“ skintone_data/labelled/train\n",
      "  âœ“ skintone_data/labelled/val\n",
      "  âœ“ skintone_data/labelled/test\n",
      "  âœ“ skintone_data/unlabelled\n",
      "  âœ“ skintone_data/metadata\n",
      "  âœ“ skintone_data/labelled/train/dark\n",
      "  âœ“ skintone_data/labelled/train/light\n",
      "  âœ“ skintone_data/labelled/train/mid-dark\n",
      "  âœ“ skintone_data/labelled/train/mid-light\n",
      "  âœ“ skintone_data/labelled/val/dark\n",
      "  âœ“ skintone_data/labelled/val/light\n",
      "  âœ“ skintone_data/labelled/val/mid-dark\n",
      "  âœ“ skintone_data/labelled/val/mid-light\n",
      "  âœ“ skintone_data/labelled/test/dark\n",
      "  âœ“ skintone_data/labelled/test/light\n",
      "  âœ“ skintone_data/labelled/test/mid-dark\n",
      "  âœ“ skintone_data/labelled/test/mid-light\n",
      "\n",
      "âš™ï¸  TRAITEMENT DES CLASSES\n",
      "------------------------------------------------------------\n",
      "\n",
      "ğŸ“Š Traitement de la classe: dark\n",
      "  Total Ã  utiliser: 6844 images\n",
      "  âœ“ LabellisÃ©: 2053 images (train: 1437, val: 307, test: 309)\n",
      "  âœ“ Non-labellisÃ©: 4791 images\n",
      "\n",
      "ğŸ“Š Traitement de la classe: light\n",
      "  Total Ã  utiliser: 6844 images\n",
      "  âœ“ LabellisÃ©: 2053 images (train: 1437, val: 307, test: 309)\n",
      "  âœ“ Non-labellisÃ©: 4791 images\n",
      "\n",
      "ğŸ“Š Traitement de la classe: mid-dark\n",
      "  Total Ã  utiliser: 6844 images\n",
      "  âœ“ LabellisÃ©: 2053 images (train: 1437, val: 307, test: 309)\n",
      "  âœ“ Non-labellisÃ©: 4791 images\n",
      "\n",
      "ğŸ“Š Traitement de la classe: mid-light\n",
      "  Total Ã  utiliser: 6844 images\n",
      "  âœ“ LabellisÃ©: 2053 images (train: 1437, val: 307, test: 309)\n",
      "  âœ“ Non-labellisÃ©: 4791 images\n",
      "\n",
      "ğŸ“ CRÃ‰ATION DES FICHIERS MÃ‰TADONNÃ‰ES\n",
      "  âœ“ statistics.txt crÃ©Ã© dans skintone_data\\metadata\n",
      "  âœ“ unlabelled_mapping.csv crÃ©Ã©\n",
      "\n",
      "============================================================\n",
      "âœ… DATASET CRÃ‰Ã‰ AVEC SUCCÃˆS!\n",
      "============================================================\n",
      "\n",
      "ğŸ“Š STATISTIQUES FINALES:\n",
      "  Images labellisÃ©es: 8,212\n",
      "  Images non-labellisÃ©es: 19,164\n",
      "  TOTAL: 27,376\n",
      "\n",
      "ğŸ“ˆ RATIOS FINAUX:\n",
      "  LabellisÃ©: 30.0% (objectif: 30%)\n",
      "  Non-labellisÃ©: 70.0% (objectif: 70%)\n",
      "\n",
      "ğŸ“ STRUCTURE CRÃ‰Ã‰E:\n",
      "  skintone_data/\n",
      "    â”œâ”€â”€ labelled/\n",
      "    â”‚   â”œâ”€â”€ train/ (images organisÃ©es par classe)\n",
      "    â”‚   â”œâ”€â”€ val/   (images organisÃ©es par classe)\n",
      "    â”‚   â””â”€â”€ test/  (images organisÃ©es par classe)\n",
      "    â”œâ”€â”€ unlabelled/ (toutes images mÃ©langÃ©es)\n",
      "    â””â”€â”€ metadata/ (fichiers statistiques)\n",
      "\n",
      "âœ¨ Votre dataset est prÃªt pour:\n",
      "   - EntraÃ®nement supervisÃ©: utiliser 'labelled/train/'\n",
      "   - MÃ©thodes semi-supervisÃ©es: utiliser 'labelled/train/' + 'unlabelled/'\n",
      "   - Ã‰valuation: utiliser 'labelled/test/'\n",
      "\n",
      "============================================================\n",
      "âœ… OpÃ©ration terminÃ©e avec succÃ¨s!\n",
      "ğŸ“ Dataset disponible dans: skintone_data\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "class SemiSupervisedDatasetCreator:\n",
    "    def __init__(self):\n",
    "        self.source_dir = \"skintone\"  # Votre dossier source\n",
    "        self.target_dir = \"skintone_data\"  # Dossier de sortie\n",
    "        self.random_seed = 42  # Pour reproductibilitÃ©\n",
    "        \n",
    "        # ParamÃ¨tres standards de recherche\n",
    "        self.TOTAL_PER_CLASS = 6844  # Nombre minimum d'images par classe\n",
    "        self.LABELLED_RATIO = 0.30   # 30% labellisÃ©\n",
    "        self.UNLABELLED_RATIO = 0.70  # 70% non-labellisÃ©\n",
    "        \n",
    "        # Split interne pour donnÃ©es labellisÃ©es\n",
    "        self.TRAIN_RATIO = 0.70      # 70% train\n",
    "        self.VAL_RATIO = 0.15        # 15% validation\n",
    "        self.TEST_RATIO = 0.15       # 15% test\n",
    "        \n",
    "        # Calcul des quantitÃ©s\n",
    "        self.calculate_counts()\n",
    "        \n",
    "        # Classes\n",
    "        self.classes = ['dark', 'light', 'mid-dark', 'mid-light']\n",
    "        \n",
    "        random.seed(self.random_seed)\n",
    "    \n",
    "    def calculate_counts(self):\n",
    "        \"\"\"Calcule les nombres d'images pour chaque split\"\"\"\n",
    "        self.labelled_per_class = int(self.TOTAL_PER_CLASS * self.LABELLED_RATIO)\n",
    "        self.unlabelled_per_class = self.TOTAL_PER_CLASS - self.labelled_per_class\n",
    "        \n",
    "        self.train_per_class = int(self.labelled_per_class * self.TRAIN_RATIO)\n",
    "        self.val_per_class = int(self.labelled_per_class * self.VAL_RATIO)\n",
    "        self.test_per_class = self.labelled_per_class - self.train_per_class - self.val_per_class\n",
    "        \n",
    "        print(\"=\" * 60)\n",
    "        print(\"CONFIGURATION DU DATASET SEMI-SUPERVISÃ‰\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Images par classe: {self.TOTAL_PER_CLASS:,}\")\n",
    "        print(f\"Ratio: {self.LABELLED_RATIO*100:.0f}% labellisÃ© / {self.UNLABELLED_RATIO*100:.0f}% non-labellisÃ©\")\n",
    "        print(f\"\\nDÃ©tails par classe:\")\n",
    "        print(f\"  LabellisÃ©: {self.labelled_per_class:,} images\")\n",
    "        print(f\"    â”œâ”€â”€ Train: {self.train_per_class:,} (70%)\")\n",
    "        print(f\"    â”œâ”€â”€ Validation: {self.val_per_class:,} (15%)\")\n",
    "        print(f\"    â””â”€â”€ Test: {self.test_per_class:,} (15%)\")\n",
    "        print(f\"  Non-labellisÃ©: {self.unlabelled_per_class:,} images\")\n",
    "        print(\"=\" * 60)\n",
    "    \n",
    "    def verify_source_data(self):\n",
    "        \"\"\"VÃ©rifie que les donnÃ©es source existent\"\"\"\n",
    "        print(\"\\nğŸ” VÃ‰RIFICATION DES DONNÃ‰ES SOURCE\")\n",
    "        \n",
    "        if not os.path.exists(self.source_dir):\n",
    "            print(f\"âŒ ERREUR: Le dossier '{self.source_dir}' n'existe pas!\")\n",
    "            print(f\"   Placez ce script dans le mÃªme dossier que 'skintone/'\")\n",
    "            return False\n",
    "        \n",
    "        # VÃ©rifier chaque classe\n",
    "        missing_classes = []\n",
    "        for cls in self.classes:\n",
    "            class_path = os.path.join(self.source_dir, cls)\n",
    "            if not os.path.exists(class_path):\n",
    "                missing_classes.append(cls)\n",
    "            else:\n",
    "                # Compter les images\n",
    "                images = [f for f in os.listdir(class_path) \n",
    "                         if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "                print(f\"  âœ“ {cls}: {len(images):,} images\")\n",
    "                \n",
    "                # VÃ©rifier si assez d'images\n",
    "                if len(images) < self.TOTAL_PER_CLASS:\n",
    "                    print(f\"    âš ï¸  Attention: {cls} a seulement {len(images)} images\")\n",
    "                    print(f\"    Nous allons utiliser {len(images)} au lieu de {self.TOTAL_PER_CLASS}\")\n",
    "        \n",
    "        if missing_classes:\n",
    "            print(f\"\\nâŒ Classes manquantes: {missing_classes}\")\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def create_directory_structure(self):\n",
    "        \"\"\"CrÃ©e la structure de dossiers cible\"\"\"\n",
    "        print(\"\\nğŸ“ CRÃ‰ATION DE LA STRUCTURE DES DOSSIERS\")\n",
    "        \n",
    "        # Dossiers principaux\n",
    "        main_dirs = [\n",
    "            f\"{self.target_dir}/labelled/train\",\n",
    "            f\"{self.target_dir}/labelled/val\", \n",
    "            f\"{self.target_dir}/labelled/test\",\n",
    "            f\"{self.target_dir}/unlabelled\",\n",
    "            f\"{self.target_dir}/metadata\"\n",
    "        ]\n",
    "        \n",
    "        # Sous-dossiers par classe\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            for cls in self.classes:\n",
    "                main_dirs.append(f\"{self.target_dir}/labelled/{split}/{cls}\")\n",
    "        \n",
    "        # CrÃ©ation\n",
    "        for dir_path in main_dirs:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "            print(f\"  âœ“ {dir_path}\")\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def process_class(self, cls):\n",
    "        \"\"\"Traite une classe spÃ©cifique\"\"\"\n",
    "        print(f\"\\nğŸ“Š Traitement de la classe: {cls}\")\n",
    "        \n",
    "        # Chemin source\n",
    "        source_path = os.path.join(self.source_dir, cls)\n",
    "        \n",
    "        # Lister toutes les images\n",
    "        all_images = [\n",
    "            f for f in os.listdir(source_path) \n",
    "            if f.lower().endswith(('.jpg', '.jpeg', '.png'))\n",
    "        ]\n",
    "        \n",
    "        # Si moins d'images que prÃ©vu, ajuster\n",
    "        available_count = len(all_images)\n",
    "        if available_count < self.TOTAL_PER_CLASS:\n",
    "            print(f\"  âš ï¸  Seulement {available_count} images disponibles\")\n",
    "            print(f\"  Ajustement du plan...\")\n",
    "            \n",
    "            # Ajuster dynamiquement\n",
    "            actual_total = available_count\n",
    "            actual_labelled = int(actual_total * self.LABELLED_RATIO)\n",
    "            actual_unlabelled = actual_total - actual_labelled\n",
    "            \n",
    "            actual_train = int(actual_labelled * self.TRAIN_RATIO)\n",
    "            actual_val = int(actual_labelled * self.VAL_RATIO)\n",
    "            actual_test = actual_labelled - actual_train - actual_val\n",
    "        else:\n",
    "            # Prendre exactement le nombre requis\n",
    "            actual_total = self.TOTAL_PER_CLASS\n",
    "            actual_labelled = self.labelled_per_class\n",
    "            actual_unlabelled = self.unlabelled_per_class\n",
    "            actual_train = self.train_per_class\n",
    "            actual_val = self.val_per_class\n",
    "            actual_test = self.test_per_class\n",
    "        \n",
    "        print(f\"  Total Ã  utiliser: {actual_total} images\")\n",
    "        \n",
    "        # Ã‰chantillonnage alÃ©atoire\n",
    "        selected_images = random.sample(all_images, min(actual_total, len(all_images)))\n",
    "        random.shuffle(selected_images)\n",
    "        \n",
    "        # Split labellisÃ© vs non-labellisÃ©\n",
    "        labelled_images = selected_images[:actual_labelled]\n",
    "        unlabelled_images = selected_images[actual_labelled:actual_total]\n",
    "        \n",
    "        # Split labellisÃ© en train/val/test\n",
    "        train_images = labelled_images[:actual_train]\n",
    "        val_images = labelled_images[actual_train:actual_train + actual_val]\n",
    "        test_images = labelled_images[actual_train + actual_val:]\n",
    "        \n",
    "        # Copier les images labellisÃ©es\n",
    "        stats = {'labelled': 0, 'unlabelled': 0}\n",
    "        \n",
    "        # Train\n",
    "        for img in train_images:\n",
    "            src = os.path.join(source_path, img)\n",
    "            dst = os.path.join(self.target_dir, 'labelled', 'train', cls, img)\n",
    "            shutil.copy2(src, dst)\n",
    "            stats['labelled'] += 1\n",
    "        \n",
    "        # Validation\n",
    "        for img in val_images:\n",
    "            src = os.path.join(source_path, img)\n",
    "            dst = os.path.join(self.target_dir, 'labelled', 'val', cls, img)\n",
    "            shutil.copy2(src, dst)\n",
    "            stats['labelled'] += 1\n",
    "        \n",
    "        # Test\n",
    "        for img in test_images:\n",
    "            src = os.path.join(source_path, img)\n",
    "            dst = os.path.join(self.target_dir, 'labelled', 'test', cls, img)\n",
    "            shutil.copy2(src, dst)\n",
    "            stats['labelled'] += 1\n",
    "        \n",
    "        # Copier les images non-labellisÃ©es avec nom unique\n",
    "        unlabelled_count = 0\n",
    "        unlabelled_dir = os.path.join(self.target_dir, 'unlabelled')\n",
    "        \n",
    "        for img in unlabelled_images:\n",
    "            src = os.path.join(source_path, img)\n",
    "            \n",
    "            # CrÃ©er un nom unique avec classe d'origine\n",
    "            img_name, img_ext = os.path.splitext(img)\n",
    "            unique_name = f\"{cls}_{img_name}_{unlabelled_count:06d}{img_ext}\"\n",
    "            dst = os.path.join(unlabelled_dir, unique_name)\n",
    "            \n",
    "            shutil.copy2(src, dst)\n",
    "            stats['unlabelled'] += 1\n",
    "            unlabelled_count += 1\n",
    "        \n",
    "        print(f\"  âœ“ LabellisÃ©: {stats['labelled']} images (train: {len(train_images)}, val: {len(val_images)}, test: {len(test_images)})\")\n",
    "        print(f\"  âœ“ Non-labellisÃ©: {stats['unlabelled']} images\")\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def create_metadata(self):\n",
    "        \"\"\"CrÃ©e des fichiers metadata avec les statistiques\"\"\"\n",
    "        print(\"\\nğŸ“ CRÃ‰ATION DES FICHIERS MÃ‰TADONNÃ‰ES\")\n",
    "        \n",
    "        metadata_dir = os.path.join(self.target_dir, 'metadata')\n",
    "        \n",
    "        # Compter les images dans chaque dossier\n",
    "        stats = {\n",
    "            'labelled': {'train': {}, 'val': {}, 'test': {}},\n",
    "            'unlabelled': 0\n",
    "        }\n",
    "        \n",
    "        # Compter labellisÃ©\n",
    "        for split in ['train', 'val', 'test']:\n",
    "            for cls in self.classes:\n",
    "                split_dir = os.path.join(self.target_dir, 'labelled', split, cls)\n",
    "                if os.path.exists(split_dir):\n",
    "                    count = len([f for f in os.listdir(split_dir) \n",
    "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    stats['labelled'][split][cls] = count\n",
    "        \n",
    "        # Compter non-labellisÃ©\n",
    "        unlabelled_dir = os.path.join(self.target_dir, 'unlabelled')\n",
    "        if os.path.exists(unlabelled_dir):\n",
    "            stats['unlabelled'] = len([f for f in os.listdir(unlabelled_dir) \n",
    "                                      if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        # Ã‰crire les statistiques\n",
    "        with open(os.path.join(metadata_dir, 'statistics.txt'), 'w') as f:\n",
    "            f.write(\"=\" * 60 + \"\\n\")\n",
    "            f.write(\"STATISTIQUES DU DATASET SEMI-SUPERVISÃ‰\\n\")\n",
    "            f.write(\"=\" * 60 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(\"CONFIGURATION:\\n\")\n",
    "            f.write(f\"Source: {self.source_dir}\\n\")\n",
    "            f.write(f\"Destination: {self.target_dir}\\n\")\n",
    "            f.write(f\"Ratio: {self.LABELLED_RATIO*100:.0f}% labellisÃ© / {self.UNLABELLED_RATIO*100:.0f}% non-labellisÃ©\\n\")\n",
    "            f.write(f\"Images par classe (cible): {self.TOTAL_PER_CLASS}\\n\\n\")\n",
    "            \n",
    "            f.write(\"RÃ‰SULTATS PAR CLASSE:\\n\")\n",
    "            f.write(\"-\" * 60 + \"\\n\")\n",
    "            \n",
    "            # Calculer les totaux\n",
    "            total_labelled = 0\n",
    "            total_unlabelled = stats['unlabelled']\n",
    "            \n",
    "            for cls in self.classes:\n",
    "                f.write(f\"\\n{cls.upper()}:\\n\")\n",
    "                \n",
    "                # LabellisÃ©\n",
    "                cls_labelled = 0\n",
    "                for split in ['train', 'val', 'test']:\n",
    "                    count = stats['labelled'][split].get(cls, 0)\n",
    "                    cls_labelled += count\n",
    "                    f.write(f\"  {split}: {count} images\\n\")\n",
    "                \n",
    "                total_labelled += cls_labelled\n",
    "                f.write(f\"  Total labellisÃ©: {cls_labelled}\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"=\" * 60 + \"\\n\")\n",
    "            f.write(\"TOTAUX GÃ‰NÃ‰RAUX:\\n\")\n",
    "            f.write(f\"  Images labellisÃ©es: {total_labelled}\\n\")\n",
    "            f.write(f\"  Images non-labellisÃ©es: {total_unlabelled}\\n\")\n",
    "            f.write(f\"  TOTAL: {total_labelled + total_unlabelled}\\n\\n\")\n",
    "            \n",
    "            f.write(\"RATIOS FINAUX:\\n\")\n",
    "            total_all = total_labelled + total_unlabelled\n",
    "            if total_all > 0:\n",
    "                f.write(f\"  LabellisÃ©: {(total_labelled/total_all*100):.1f}%\\n\")\n",
    "                f.write(f\"  Non-labellisÃ©: {(total_unlabelled/total_all*100):.1f}%\\n\")\n",
    "        \n",
    "        print(f\"  âœ“ statistics.txt crÃ©Ã© dans {metadata_dir}\")\n",
    "        \n",
    "        # CrÃ©er un fichier CSV pour le mapping non-labellisÃ©\n",
    "        with open(os.path.join(metadata_dir, 'unlabelled_mapping.csv'), 'w') as f:\n",
    "            f.write(\"filename,original_class\\n\")\n",
    "            \n",
    "            unlabelled_dir = os.path.join(self.target_dir, 'unlabelled')\n",
    "            if os.path.exists(unlabelled_dir):\n",
    "                for filename in os.listdir(unlabelled_dir):\n",
    "                    if filename.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                        # Extraire la classe d'origine du nom de fichier\n",
    "                        parts = filename.split('_')\n",
    "                        if len(parts) >= 1:\n",
    "                            original_class = parts[0]\n",
    "                            f.write(f\"{filename},{original_class}\\n\")\n",
    "        \n",
    "        print(f\"  âœ“ unlabelled_mapping.csv crÃ©Ã©\")\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"ExÃ©cute le pipeline complet\"\"\"\n",
    "        print(\"ğŸš€ DÃ‰BUT DE LA CRÃ‰ATION DU DATASET SEMI-SUPERVISÃ‰\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Ã‰tape 1: VÃ©rification\n",
    "        if not self.verify_source_data():\n",
    "            print(\"\\nâŒ Impossible de continuer. VÃ©rifiez vos donnÃ©es source.\")\n",
    "            return False\n",
    "        \n",
    "        # Ã‰tape 2: CrÃ©ation structure\n",
    "        self.create_directory_structure()\n",
    "        \n",
    "        # Ã‰tape 3: Traitement de chaque classe\n",
    "        print(\"\\nâš™ï¸  TRAITEMENT DES CLASSES\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        all_stats = {}\n",
    "        for cls in self.classes:\n",
    "            all_stats[cls] = self.process_class(cls)\n",
    "        \n",
    "        # Ã‰tape 4: Metadata\n",
    "        self.create_metadata()\n",
    "        \n",
    "        # Ã‰tape 5: RÃ©sumÃ© final\n",
    "        self.print_summary()\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def print_summary(self):\n",
    "        \"\"\"Affiche un rÃ©sumÃ© final\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"âœ… DATASET CRÃ‰Ã‰ AVEC SUCCÃˆS!\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Calcul des totaux\n",
    "        total_labelled = 0\n",
    "        total_unlabelled = 0\n",
    "        \n",
    "        for split in ['train', 'val', 'test']:\n",
    "            split_dir = os.path.join(self.target_dir, 'labelled', split)\n",
    "            for cls in self.classes:\n",
    "                class_dir = os.path.join(split_dir, cls)\n",
    "                if os.path.exists(class_dir):\n",
    "                    count = len([f for f in os.listdir(class_dir) \n",
    "                               if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "                    total_labelled += count\n",
    "        \n",
    "        unlabelled_dir = os.path.join(self.target_dir, 'unlabelled')\n",
    "        if os.path.exists(unlabelled_dir):\n",
    "            total_unlabelled = len([f for f in os.listdir(unlabelled_dir) \n",
    "                                  if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "        \n",
    "        total_all = total_labelled + total_unlabelled\n",
    "        \n",
    "        print(f\"\\nğŸ“Š STATISTIQUES FINALES:\")\n",
    "        print(f\"  Images labellisÃ©es: {total_labelled:,}\")\n",
    "        print(f\"  Images non-labellisÃ©es: {total_unlabelled:,}\")\n",
    "        print(f\"  TOTAL: {total_all:,}\")\n",
    "        \n",
    "        if total_all > 0:\n",
    "            print(f\"\\nğŸ“ˆ RATIOS FINAUX:\")\n",
    "            print(f\"  LabellisÃ©: {(total_labelled/total_all*100):.1f}% (objectif: 30%)\")\n",
    "            print(f\"  Non-labellisÃ©: {(total_unlabelled/total_all*100):.1f}% (objectif: 70%)\")\n",
    "        \n",
    "        print(f\"\\nğŸ“ STRUCTURE CRÃ‰Ã‰E:\")\n",
    "        print(f\"  {self.target_dir}/\")\n",
    "        print(f\"    â”œâ”€â”€ labelled/\")\n",
    "        print(f\"    â”‚   â”œâ”€â”€ train/ (images organisÃ©es par classe)\")\n",
    "        print(f\"    â”‚   â”œâ”€â”€ val/   (images organisÃ©es par classe)\")\n",
    "        print(f\"    â”‚   â””â”€â”€ test/  (images organisÃ©es par classe)\")\n",
    "        print(f\"    â”œâ”€â”€ unlabelled/ (toutes images mÃ©langÃ©es)\")\n",
    "        print(f\"    â””â”€â”€ metadata/ (fichiers statistiques)\")\n",
    "        \n",
    "        print(f\"\\nâœ¨ Votre dataset est prÃªt pour:\")\n",
    "        print(f\"   - EntraÃ®nement supervisÃ©: utiliser 'labelled/train/'\")\n",
    "        print(f\"   - MÃ©thodes semi-supervisÃ©es: utiliser 'labelled/train/' + 'unlabelled/'\")\n",
    "        print(f\"   - Ã‰valuation: utiliser 'labelled/test/'\")\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"ğŸ¯ CRÃ‰ATEUR DE DATASET SEMI-SUPERVISÃ‰\")\n",
    "    print(\"Ratio: 30% labellisÃ© / 70% non-labellisÃ©\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # CrÃ©er l'instance\n",
    "    creator = SemiSupervisedDatasetCreator()\n",
    "    \n",
    "    # ExÃ©cuter\n",
    "    success = creator.run()\n",
    "    \n",
    "    if success:\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"âœ… OpÃ©ration terminÃ©e avec succÃ¨s!\")\n",
    "        print(f\"ğŸ“ Dataset disponible dans: {creator.target_dir}\")\n",
    "        print(\"=\" * 60)\n",
    "    else:\n",
    "        print(\"\\nâŒ Ã‰chec de la crÃ©ation du dataset.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    INSTRUCTIONS:\n",
    "    1. Placez ce script dans le mÃªme dossier que 'skintone/'\n",
    "    2. ExÃ©cutez: python create_semi_supervised_dataset.py\n",
    "    3. Attendez la fin du traitement\n",
    "    \n",
    "    STRUCTURE ATTENDUE EN ENTRÃ‰E:\n",
    "    skintone/\n",
    "        â”œâ”€â”€ dark/           (images .jpg/.png)\n",
    "        â”œâ”€â”€ light/          (images .jpg/.png)\n",
    "        â”œâ”€â”€ mid-dark/       (images .jpg/.png)\n",
    "        â””â”€â”€ mid-light/      (images .jpg/.png)\n",
    "    \"\"\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed82d8fd-7ea1-45bd-9dab-2231f0d8cb3b",
   "metadata": {},
   "source": [
    "# ğŸ¯ Ã‰TAPE 2 : PRÃ‰TRAITEMENT POUR MACHINE LEARNING CLASSIQUE\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f3ac65-52c3-4d1b-b251-116fca458088",
   "metadata": {},
   "source": [
    "!pip install opencv-python scikit-image pandas numpy tqdm torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea15e569-35cf-4809-861a-4305cb3fcaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ PRÃ‰PROCESSING POUR APPRENTISSAGE AUTOMATIQUE SEMI-SUPERVISÃ‰\n",
      "============================================================\n",
      "ğŸ”§ PRÃ‰TRAITEMENT DES DONNÃ‰ES\n",
      "============================================================\n",
      "ğŸ“¥ Chargement des donnÃ©es labellisÃ©es (train)...\n",
      "  Traitement classe dark...\n",
      "  Traitement classe light...\n",
      "  Traitement classe mid-dark...\n",
      "  Traitement classe mid-light...\n",
      "  âœ“ 5748 Ã©chantillons chargÃ©s\n",
      "ğŸ“¥ Chargement des donnÃ©es labellisÃ©es (val)...\n",
      "  Traitement classe dark...\n",
      "  Traitement classe light...\n",
      "  Traitement classe mid-dark...\n",
      "  Traitement classe mid-light...\n",
      "  âœ“ 1228 Ã©chantillons chargÃ©s\n",
      "ğŸ“¥ Chargement des donnÃ©es labellisÃ©es (test)...\n",
      "  Traitement classe dark...\n",
      "  Traitement classe light...\n",
      "  Traitement classe mid-dark...\n",
      "  Traitement classe mid-light...\n",
      "  âœ“ 1236 Ã©chantillons chargÃ©s\n",
      "ğŸ“¥ Chargement des donnÃ©es non-labellisÃ©es...\n",
      "  âœ“ 19164 Ã©chantillons non-labellisÃ©s chargÃ©s\n",
      "ğŸ“Š Ajustement du scaler et PCA...\n",
      "  Variance expliquÃ©e par PCA: [0.05828715 0.03958944 0.02882378 0.02233959 0.02217248] (top 5)\n",
      "  Variance totale expliquÃ©e: 0.539\n",
      "âœ… PrÃ©traitement terminÃ©!\n",
      "\n",
      "ğŸ“Š STATISTIQUES DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES:\n",
      "  train: 5748 Ã©chantillons, 100 features\n",
      "    Classes: {np.int64(0): np.int64(1437), np.int64(1): np.int64(1437), np.int64(2): np.int64(1437), np.int64(3): np.int64(1437)}\n",
      "  val: 1228 Ã©chantillons, 100 features\n",
      "    Classes: {np.int64(0): np.int64(307), np.int64(1): np.int64(307), np.int64(2): np.int64(307), np.int64(3): np.int64(307)}\n",
      "  test: 1236 Ã©chantillons, 100 features\n",
      "    Classes: {np.int64(0): np.int64(309), np.int64(1): np.int64(309), np.int64(2): np.int64(309), np.int64(3): np.int64(309)}\n",
      "  unlabelled: 19164 Ã©chantillons, 100 features\n",
      "ğŸ’¾ Sauvegarde dans preprocessed_ml_data...\n",
      "  âœ“ DonnÃ©es sauvegardÃ©es\n",
      "\n",
      "âœ… PrÃ©processing terminÃ©!\n",
      "Vous pouvez maintenant utiliser ces donnÃ©es pour vos algorithmes ML semi-supervisÃ©s.\n",
      "\n",
      "ğŸ“ Fichiers crÃ©Ã©s:\n",
      "  - preprocessed_ml_data/train_data.npz\n",
      "  - preprocessed_ml_data/val_data.npz\n",
      "  - preprocessed_ml_data/test_data.npz\n",
      "  - preprocessed_ml_data/unlabelled_data.npz\n",
      "  - preprocessed_ml_data/scaler.pkl\n",
      "  - preprocessed_ml_data/pca.pkl\n",
      "  - preprocessed_ml_data/metadata.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class MLSkinTonePreprocessor:\n",
    "    \"\"\"PrÃ©processeur pour apprentissage automatique semi-supervisÃ© sur donnÃ©es d'images de teinte de peau\"\"\"\n",
    "    \n",
    "    def __init__(self, root_dir='skintone_data', feature_type='hog', n_components=100):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (str): Chemin vers le dossier skintone_data\n",
    "            feature_type (str): Type de features ('hog', 'sift', 'color_hist', 'combined')\n",
    "            n_components (int): Nombre de composantes pour PCA\n",
    "        \"\"\"\n",
    "        self.root_dir = Path(root_dir)\n",
    "        self.feature_type = feature_type\n",
    "        self.n_components = n_components\n",
    "        self.scaler = StandardScaler()\n",
    "        self.pca = PCA(n_components=n_components, random_state=42)\n",
    "        \n",
    "        # Classes\n",
    "        self.classes = ['dark', 'light', 'mid-dark', 'mid-light']\n",
    "        self.class_to_idx = {cls: idx for idx, cls in enumerate(self.classes)}\n",
    "        self.idx_to_class = {idx: cls for cls, idx in self.class_to_idx.items()}\n",
    "        \n",
    "        # ParamÃ¨tres pour extraction de features\n",
    "        self.target_size = (128, 128)  # Taille pour redimensionnement\n",
    "    \n",
    "    def extract_hog_features(self, image):\n",
    "        \"\"\"Extrait les features HOG d'une image\"\"\"\n",
    "        # Convertir en niveaux de gris\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Calculer HOG\n",
    "        hog = cv2.HOGDescriptor()\n",
    "        features = hog.compute(gray)\n",
    "        \n",
    "        return features.flatten()\n",
    "    \n",
    "    def extract_sift_features(self, image):\n",
    "        \"\"\"Extrait les features SIFT d'une image\"\"\"\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        # Initialiser SIFT\n",
    "        sift = cv2.SIFT_create()\n",
    "        keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "        \n",
    "        if descriptors is None:\n",
    "            # Si pas de keypoints, retourner un vecteur nul\n",
    "            return np.zeros(128)  # SIFT produit 128 dimensions par dÃ©faut\n",
    "        \n",
    "        # Utiliser la moyenne des descripteurs\n",
    "        return np.mean(descriptors, axis=0)\n",
    "    \n",
    "    def extract_color_histogram(self, image, bins=32):\n",
    "        \"\"\"Extrait l'histogramme de couleur d'une image\"\"\"\n",
    "        # Calculer histogramme pour chaque canal\n",
    "        hist_r = cv2.calcHist([image], [0], None, [bins], [0, 256])\n",
    "        hist_g = cv2.calcHist([image], [1], None, [bins], [0, 256])\n",
    "        hist_b = cv2.calcHist([image], [2], None, [bins], [0, 256])\n",
    "        \n",
    "        # ConcatÃ©ner et normaliser\n",
    "        hist = np.concatenate([hist_r, hist_g, hist_b]).flatten()\n",
    "        hist = cv2.normalize(hist, hist).flatten()\n",
    "        \n",
    "        return hist\n",
    "    \n",
    "    def extract_combined_features(self, image):\n",
    "        \"\"\"Extrait des features combinÃ©es (HOG + histogramme de couleur)\"\"\"\n",
    "        hog_features = self.extract_hog_features(image)\n",
    "        color_features = self.extract_color_histogram(image)\n",
    "        \n",
    "        return np.concatenate([hog_features, color_features])\n",
    "    \n",
    "    def extract_features(self, image_path):\n",
    "        \"\"\"Extrait les features d'une image selon le type choisi\"\"\"\n",
    "        # Charger et redimensionner l'image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = np.array(image)\n",
    "        image = cv2.resize(image, self.target_size)\n",
    "        \n",
    "        if self.feature_type == 'hog':\n",
    "            return self.extract_hog_features(image)\n",
    "        elif self.feature_type == 'sift':\n",
    "            return self.extract_sift_features(image)\n",
    "        elif self.feature_type == 'color_hist':\n",
    "            return self.extract_color_histogram(image)\n",
    "        elif self.feature_type == 'combined':\n",
    "            return self.extract_combined_features(image)\n",
    "        else:\n",
    "            raise ValueError(f\"Type de feature non supportÃ©: {self.feature_type}\")\n",
    "    \n",
    "    def load_labelled_data(self, split='train'):\n",
    "        \"\"\"Charge et traite les donnÃ©es labellisÃ©es\"\"\"\n",
    "        print(f\"ğŸ“¥ Chargement des donnÃ©es labellisÃ©es ({split})...\")\n",
    "        \n",
    "        features_list = []\n",
    "        labels_list = []\n",
    "        filenames = []\n",
    "        \n",
    "        split_dir = self.root_dir / 'labelled' / split\n",
    "        \n",
    "        for cls in self.classes:\n",
    "            class_dir = split_dir / cls\n",
    "            if class_dir.exists():\n",
    "                print(f\"  Traitement classe {cls}...\")\n",
    "                \n",
    "                for img_path in class_dir.glob('*.jpg'):\n",
    "                    try:\n",
    "                        features = self.extract_features(img_path)\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(self.class_to_idx[cls])\n",
    "                        filenames.append(img_path.name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "                \n",
    "                for img_path in class_dir.glob('*.jpeg'):\n",
    "                    try:\n",
    "                        features = self.extract_features(img_path)\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(self.class_to_idx[cls])\n",
    "                        filenames.append(img_path.name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "                \n",
    "                for img_path in class_dir.glob('*.png'):\n",
    "                    try:\n",
    "                        features = self.extract_features(img_path)\n",
    "                        features_list.append(features)\n",
    "                        labels_list.append(self.class_to_idx[cls])\n",
    "                        filenames.append(img_path.name)\n",
    "                    except Exception as e:\n",
    "                        print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        y = np.array(labels_list)\n",
    "        \n",
    "        print(f\"  âœ“ {len(X)} Ã©chantillons chargÃ©s\")\n",
    "        \n",
    "        return X, y, filenames\n",
    "    \n",
    "    def load_unlabelled_data(self):\n",
    "        \"\"\"Charge et traite les donnÃ©es non-labellisÃ©es\"\"\"\n",
    "        print(\"ğŸ“¥ Chargement des donnÃ©es non-labellisÃ©es...\")\n",
    "        \n",
    "        features_list = []\n",
    "        filenames = []\n",
    "        original_classes = []\n",
    "        \n",
    "        unlabelled_dir = self.root_dir / 'unlabelled'\n",
    "        mapping_file = self.root_dir / 'metadata' / 'unlabelled_mapping.csv'\n",
    "        \n",
    "        # Charger le mapping si disponible\n",
    "        mapping_dict = {}\n",
    "        if mapping_file.exists():\n",
    "            mapping_df = pd.read_csv(mapping_file)\n",
    "            mapping_dict = dict(zip(mapping_df['filename'], mapping_df['original_class']))\n",
    "        \n",
    "        if unlabelled_dir.exists():\n",
    "            for img_path in unlabelled_dir.glob('*.jpg'):\n",
    "                try:\n",
    "                    features = self.extract_features(img_path)\n",
    "                    features_list.append(features)\n",
    "                    filenames.append(img_path.name)\n",
    "                    original_classes.append(mapping_dict.get(img_path.name, 'unknown'))\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "            \n",
    "            for img_path in unlabelled_dir.glob('*.jpeg'):\n",
    "                try:\n",
    "                    features = self.extract_features(img_path)\n",
    "                    features_list.append(features)\n",
    "                    filenames.append(img_path.name)\n",
    "                    original_classes.append(mapping_dict.get(img_path.name, 'unknown'))\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "            \n",
    "            for img_path in unlabelled_dir.glob('*.png'):\n",
    "                try:\n",
    "                    features = self.extract_features(img_path)\n",
    "                    features_list.append(features)\n",
    "                    filenames.append(img_path.name)\n",
    "                    original_classes.append(mapping_dict.get(img_path.name, 'unknown'))\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ Erreur avec {img_path.name}: {e}\")\n",
    "        \n",
    "        X = np.array(features_list)\n",
    "        \n",
    "        print(f\"  âœ“ {len(X)} Ã©chantillons non-labellisÃ©s chargÃ©s\")\n",
    "        \n",
    "        return X, filenames, original_classes\n",
    "    \n",
    "    def preprocess_data(self):\n",
    "        \"\"\"PrÃ©traite toutes les donnÃ©es et applique PCA + standardisation\"\"\"\n",
    "        print(\"ğŸ”§ PRÃ‰TRAITEMENT DES DONNÃ‰ES\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Charger les donnÃ©es\n",
    "        X_train_labelled, y_train_labelled, train_filenames = self.load_labelled_data('train')\n",
    "        X_val, y_val, val_filenames = self.load_labelled_data('val')\n",
    "        X_test, y_test, test_filenames = self.load_labelled_data('test')\n",
    "        X_unlabelled, unlabelled_filenames, unlabelled_classes = self.load_unlabelled_data()\n",
    "        \n",
    "        # Combiner toutes les features pour ajuster le scaler et PCA\n",
    "        print(\"ğŸ“Š Ajustement du scaler et PCA...\")\n",
    "        all_features = np.vstack([X_train_labelled, X_val, X_test, X_unlabelled])\n",
    "        \n",
    "        # Standardisation\n",
    "        all_features_scaled = self.scaler.fit_transform(all_features)\n",
    "        \n",
    "        # PCA\n",
    "        all_features_pca = self.pca.fit_transform(all_features_scaled)\n",
    "        \n",
    "        print(f\"  Variance expliquÃ©e par PCA: {self.pca.explained_variance_ratio_[:5]} (top 5)\")\n",
    "        print(f\"  Variance totale expliquÃ©e: {np.sum(self.pca.explained_variance_ratio_):.3f}\")\n",
    "        \n",
    "        # SÃ©parer les donnÃ©es transformÃ©es\n",
    "        n_train = len(X_train_labelled)\n",
    "        n_val = len(X_val)\n",
    "        n_test = len(X_test)\n",
    "        \n",
    "        X_train_pca = all_features_pca[:n_train]\n",
    "        X_val_pca = all_features_pca[n_train:n_train+n_val]\n",
    "        X_test_pca = all_features_pca[n_train+n_val:n_train+n_val+n_test]\n",
    "        X_unlabelled_pca = all_features_pca[n_train+n_val+n_test:]\n",
    "        \n",
    "        # CrÃ©er les datasets finaux\n",
    "        datasets = {\n",
    "            'train': {\n",
    "                'X': X_train_pca,\n",
    "                'y': y_train_labelled,\n",
    "                'filenames': train_filenames\n",
    "            },\n",
    "            'val': {\n",
    "                'X': X_val_pca,\n",
    "                'y': y_val,\n",
    "                'filenames': val_filenames\n",
    "            },\n",
    "            'test': {\n",
    "                'X': X_test_pca,\n",
    "                'y': y_test,\n",
    "                'filenames': test_filenames\n",
    "            },\n",
    "            'unlabelled': {\n",
    "                'X': X_unlabelled_pca,\n",
    "                'filenames': unlabelled_filenames,\n",
    "                'original_classes': unlabelled_classes\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        print(\"âœ… PrÃ©traitement terminÃ©!\")\n",
    "        \n",
    "        return datasets\n",
    "    \n",
    "    def save_preprocessed_data(self, datasets, output_dir='preprocessed_ml_data'):\n",
    "        \"\"\"Sauvegarde les donnÃ©es prÃ©traitÃ©es\"\"\"\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        print(f\"ğŸ’¾ Sauvegarde dans {output_dir}...\")\n",
    "        \n",
    "        # Sauvegarder les datasets\n",
    "        for split, data in datasets.items():\n",
    "            np.savez_compressed(\n",
    "                os.path.join(output_dir, f'{split}_data.npz'),\n",
    "                X=data['X'],\n",
    "                y=data.get('y', np.full(len(data['X']), -1)),  # -1 pour non-labellisÃ©\n",
    "                filenames=data['filenames']\n",
    "            )\n",
    "        \n",
    "        # Sauvegarder le scaler et PCA\n",
    "        with open(os.path.join(output_dir, 'scaler.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.scaler, f)\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'pca.pkl'), 'wb') as f:\n",
    "            pickle.dump(self.pca, f)\n",
    "        \n",
    "        # Sauvegarder les mÃ©tadonnÃ©es\n",
    "        metadata = {\n",
    "            'feature_type': self.feature_type,\n",
    "            'n_components': self.n_components,\n",
    "            'classes': self.classes,\n",
    "            'target_size': self.target_size,\n",
    "            'splits_info': {split: len(data['X']) for split, data in datasets.items()}\n",
    "        }\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'metadata.pkl'), 'wb') as f:\n",
    "            pickle.dump(metadata, f)\n",
    "        \n",
    "        print(\"  âœ“ DonnÃ©es sauvegardÃ©es\")\n",
    "    \n",
    "    def load_preprocessed_data(self, input_dir='preprocessed_ml_data'):\n",
    "        \"\"\"Charge les donnÃ©es prÃ©traitÃ©es\"\"\"\n",
    "        print(f\"ğŸ“‚ Chargement des donnÃ©es depuis {input_dir}...\")\n",
    "        \n",
    "        datasets = {}\n",
    "        \n",
    "        for split in ['train', 'val', 'test', 'unlabelled']:\n",
    "            data = np.load(os.path.join(input_dir, f'{split}_data.npz'))\n",
    "            datasets[split] = {\n",
    "                'X': data['X'],\n",
    "                'y': data['y'],\n",
    "                'filenames': data['filenames']\n",
    "            }\n",
    "        \n",
    "        # Charger scaler et PCA\n",
    "        with open(os.path.join(input_dir, 'scaler.pkl'), 'rb') as f:\n",
    "            self.scaler = pickle.load(f)\n",
    "        \n",
    "        with open(os.path.join(input_dir, 'pca.pkl'), 'rb') as f:\n",
    "            self.pca = pickle.load(f)\n",
    "        \n",
    "        # Charger mÃ©tadonnÃ©es\n",
    "        with open(os.path.join(input_dir, 'metadata.pkl'), 'rb') as f:\n",
    "            metadata = pickle.load(f)\n",
    "        \n",
    "        print(\"  âœ“ DonnÃ©es chargÃ©es\")\n",
    "        \n",
    "        return datasets, metadata\n",
    "\n",
    "def main():\n",
    "    \"\"\"Fonction principale\"\"\"\n",
    "    print(\"ğŸ¯ PRÃ‰PROCESSING POUR APPRENTISSAGE AUTOMATIQUE SEMI-SUPERVISÃ‰\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # CrÃ©er le prÃ©processeur\n",
    "    preprocessor = MLSkinTonePreprocessor(\n",
    "        root_dir='skintone_data',\n",
    "        feature_type='combined',  # 'hog', 'sift', 'color_hist', 'combined'\n",
    "        n_components=100\n",
    "    )\n",
    "    \n",
    "    # PrÃ©traite les donnÃ©es\n",
    "    datasets = preprocessor.preprocess_data()\n",
    "    \n",
    "    # Afficher les statistiques\n",
    "    print(\"\\nğŸ“Š STATISTIQUES DES DONNÃ‰ES PRÃ‰TRAITÃ‰ES:\")\n",
    "    for split, data in datasets.items():\n",
    "        n_samples = len(data['X'])\n",
    "        n_features = data['X'].shape[1]\n",
    "        print(f\"  {split}: {n_samples} Ã©chantillons, {n_features} features\")\n",
    "        if 'y' in data and split != 'unlabelled':\n",
    "            unique_labels, counts = np.unique(data['y'], return_counts=True)\n",
    "            print(f\"    Classes: {dict(zip(unique_labels, counts))}\")\n",
    "    \n",
    "    # Sauvegarder les donnÃ©es\n",
    "    preprocessor.save_preprocessed_data(datasets, output_dir='preprocessed_ml_data')\n",
    "    \n",
    "    print(\"\\nâœ… PrÃ©processing terminÃ©!\")\n",
    "    print(\"Vous pouvez maintenant utiliser ces donnÃ©es pour vos algorithmes ML semi-supervisÃ©s.\")\n",
    "    print(\"\\nğŸ“ Fichiers crÃ©Ã©s:\")\n",
    "    print(\"  - preprocessed_ml_data/train_data.npz\")\n",
    "    print(\"  - preprocessed_ml_data/val_data.npz\")\n",
    "    print(\"  - preprocessed_ml_data/test_data.npz\")\n",
    "    print(\"  - preprocessed_ml_data/unlabelled_data.npz\")\n",
    "    print(\"  - preprocessed_ml_data/scaler.pkl\")\n",
    "    print(\"  - preprocessed_ml_data/pca.pkl\")\n",
    "    print(\"  - preprocessed_ml_data/metadata.pkl\")\n",
    "    \n",
    "    return datasets\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    \"\"\"\n",
    "    INSTRUCTIONS:\n",
    "    1. Assurez-vous que le dossier 'skintone_data' existe (aprÃ¨s avoir exÃ©cutÃ© le script de split)\n",
    "    2. ExÃ©cutez ce code dans un notebook Jupyter\n",
    "    3. Les donnÃ©es prÃ©traitÃ©es seront sauvegardÃ©es dans 'preprocessed_ml_data/'\n",
    "    \n",
    "    TYPES DE FEATURES DISPONIBLES:\n",
    "    - 'hog': Histogram of Oriented Gradients\n",
    "    - 'sift': Scale-Invariant Feature Transform\n",
    "    - 'color_hist': Histogramme de couleur\n",
    "    - 'combined': HOG + histogramme de couleur\n",
    "    \n",
    "    UTILISATION POUR ALGORITHMES SEMI-SUPERVISÃ‰S:\n",
    "    - Pseudo-labeling: Utilisez train + unlabelled avec un classifieur de base\n",
    "    - Graph-based: Utilisez sklearn.semi_supervised.LabelPropagation\n",
    "    - Consistency regularization: Peut Ãªtre adaptÃ© avec des mÃ©thodes d'ensemble\n",
    "    \"\"\"\n",
    "    datasets = main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e750938f-5d62-41dc-8524-0131d954e3de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
